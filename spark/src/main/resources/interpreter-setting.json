[
  {
    "group": "spark",
    "name": "spark",
    "className": "org.apache.zeppelin.spark.SparkInterpreter",
    "properties": {
      "spark.executor.memory": {
        "envName": null,
        "propertyName": "spark.executor.memory",
        "defaultValue": "1g",
        "description": "Executor memory per worker instance. ex) 512m, 32g"
      },
      "spark.executor.cores": {
        "envName": null,
        "propertyName": "spark.executor.cores",
        "defaultValue": "1",
        "description": "total number of cores to use per executor"
      },
      "spark.executor.instances": {
        "envName": null,
        "propertyName": "spark.executor.instances",
        "defaultValue": "2",
        "description": "Min executors to start spark application with"
      },
      "spark.driver.memory": {
        "envName": null,
        "propertyName": "spark.driver.memory",
        "defaultValue": "2g",
        "description": "Specify driver memory. Eg: 2g for memory in gbs or 2000m for memory in mbs. \
                        If you dont use a unit, default unit is assumed to be gbs."
      },
      "spark.qubole.max.executors": {
        "envName": null,
        "propertyName": "spark.qubole.max.executors",
        "defaultValue": "10",
        "description": "Max executors autoscaling can reach to"
      },
      "spark.qubole.idle.timeout": {
        "envName": null,
        "propertyName": "spark.qubole.idle.timeout",
        "defaultValue": "10",
        "description": "spark context will stop automatically if it has not run anything since \
                        last spark.qubole.idle.timeout minutes. Set to -1 to never stop spark. \
                        This is necessary to free up cluster resources."
      },
      "zeppelin.default.interpreter": {
        "envName": null,
        "propertyName": "zeppelin.default.interpreter",
        "defaultValue": "spark",
        "description": "Default interpreter used for the spark group"
      },
      "zeppelin.interpreter.persistent": {
        "envName": null,
        "propertyName": "zeppelin.interpreter.persistent",
        "defaultValue": "false",
        "description": "Interpreter will always be up if true"
      },
      "zeppelin.interpreter.bootstrap.notebook": {
        "envName": null,
        "propertyName": "zeppelin.interpreter.bootstrap.notebook",
        "defaultValue": "null",
        "description": "Will run the notebook (with this id) \
                        before anything using this interpreter is run"
      },
      "args": {
        "envName": null,
        "propertyName": null,
        "defaultValue": "",
        "description": "spark commandline args"
      },
      "zeppelin.spark.useHiveContext": {
        "envName": "ZEPPELIN_SPARK_USEHIVECONTEXT",
        "propertyName": "zeppelin.spark.useHiveContext",
        "defaultValue": "true",
        "description": "Use HiveContext instead of SQLContext if it is true."
      },
      "spark.app.name": {
        "envName": "SPARK_APP_NAME",

        "propertyName": "spark.app.name",
        "defaultValue": "Zeppelin",
        "description": "The name of spark application."
      },
      "zeppelin.spark.printREPLOutput": {
        "envName": null,
        "propertyName": null,
        "defaultValue": "true",
        "description": "Print REPL output"
      },
      "spark.cores.max": {
        "envName": null,
        "propertyName": "spark.cores.max",
        "defaultValue": "",
        "description": "Total number of cores to use. Empty value uses all available core."
      },
      "zeppelin.spark.maxResult": {
        "envName": "ZEPPELIN_SPARK_MAXRESULT",
        "propertyName": "zeppelin.spark.maxResult",
        "defaultValue": "1000",
        "description": "Max number of SparkSQL result to display."
      },
      "master": {
        "envName": "MASTER",
        "propertyName": "spark.master",
        "defaultValue": "local[*]",
        "description": "Spark master uri. ex) spark://masterhost:7077"
      },
      "zeppelin.interpreter.persistent": {
        "envName": "ZEPPELIN_INTERPRETER_PERSISTENT",
        "propertyName": "zeppelin.interpreter.persistent",
        "defaultValue": "false",
        "description": "Interpreter will always be up if true"
      },
      "zeppelin.interpreter.bootstrap.notebook": {
        "envName": "ZEPPELIN_INTERPRETER_BOOTSTRAP_NOTEBOOK",
        "propertyName": "zeppelin.interpreter.bootstrap.notebook",
        "defaultValue": "null",
        "description": "Will run the notebook (with this id) before anything using this interpreter is run"
      }
    }
  },
  {
    "group": "spark",
    "name": "sql",
    "className": "org.apache.zeppelin.spark.SparkSqlInterpreter",
    "properties": {
      "zeppelin.spark.concurrentSQL": {
        "envName": "ZEPPELIN_SPARK_CONCURRENTSQL",
        "propertyName": "zeppelin.spark.concurrentSQL",
        "defaultValue": "false",
        "description": "Execute multiple SQL concurrently if set true."
      },
      "zeppelin.spark.sql.stacktrace": {
        "envName": "ZEPPELIN_SPARK_SQL_STACKTRACE",
        "propertyName": "zeppelin.spark.sql.stacktrace",
        "defaultValue": "false",
        "description": "Show full exception stacktrace for SQL queries if set to true."
      },
      "zeppelin.spark.maxResult": {
        "envName": "ZEPPELIN_SPARK_MAXRESULT",
        "propertyName": "zeppelin.spark.maxResult",
        "defaultValue": "1000",
        "description": "Max number of SparkSQL result to display."
      },
      "zeppelin.spark.importImplicit": {
        "envName": "ZEPPELIN_SPARK_IMPORTIMPLICIT",
        "propertyName": "zeppelin.spark.importImplicit",
        "defaultValue": "true",
        "description": "Import implicits, UDF collection, and sql if set true. true by default."
      },
      "zeppelin.spark.sql.maxConcurrency": {
        "propertName": "zeppelin.spark.sql.maxConcurrency",
        "defaultValue": 10,
        "description": "Max concurrency for spark sql if zeppelin.spark.concurrentSQL is set to true."
      }
    }
  },
  {
    "group": "spark",
    "name": "dep",
    "className": "org.apache.zeppelin.spark.DepInterpreter",
    "properties": {
      "zeppelin.dep.localrepo": {
        "envName": "ZEPPELIN_DEP_LOCALREPO",
        "propertyName": null,
        "defaultValue": "local-repo",
        "description": "local repository for dependency loader"
      },
      "zeppelin.dep.additionalRemoteRepository": {
        "envName": null,
        "propertyName": null,
        "defaultValue": "spark-packages,http://dl.bintray.com/spark-packages/maven,false;",
        "description": "A list of 'id,remote-repository-URL,is-snapshot;' for each remote repository."
      }
    }
  },
  {
    "group": "spark",
    "name": "pyspark",
    "className": "org.apache.zeppelin.spark.PySparkInterpreter",
    "properties": {
      "zeppelin.pyspark.python": {
        "envName": "PYSPARK_PYTHON",
        "propertyName": null,
        "defaultValue": "python",
        "description": "Python command to run pyspark with"
      }
    }
  }
]
